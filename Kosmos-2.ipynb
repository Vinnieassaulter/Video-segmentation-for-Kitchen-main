{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04855739f4e24c7c9353d8b99f2fd583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde3eeaac9a94434ab8008e0fb427e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)iguration_kosmos2.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- configuration_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e01a79abcd42688a4f375b8370793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/modeling_kosmos2.py:   0%|          | 0.00/77.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- modeling_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeeedbe2da944e1dbf583dae78a45549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/6.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e14fd90d22413687d75d8eb80bd57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a7353f8e04f75b169fbc059d31e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessing_kosmos2.py:   0%|          | 0.00/26.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- processing_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7763a73990084dd3b88df061be5d74db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessing_kosmos2.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- image_processing_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75ecd2a9a9d408aa100c71c8750a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65c4c7a85374dc695691667c512d557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tion_kosmos2_fast.py:   0%|          | 0.00/9.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd84587cbfc84ba198b307d5f7b819b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_kosmos2.py:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- tokenization_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/ydshieh/kosmos-2-patch14-224:\n",
      "- tokenization_kosmos2_fast.py\n",
      "- tokenization_kosmos2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Kosmos2Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoProcessor, AutoModelForVision2Seq\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m AutoModelForVision2Seq\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mydshieh/kosmos-2-patch14-224\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mydshieh/kosmos-2-patch14-224\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<grounding>An image of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:265\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         processor_class \u001b[39m=\u001b[39m processor_class_from_name(processor_class)\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m processor_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    266\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[39m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/processing_utils.py:184\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    155\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m    Instantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m            [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/processing_utils.py:228\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 228\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:680\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m         class_ref \u001b[39m=\u001b[39m tokenizer_auto_map[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 680\u001b[0m     tokenizer_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    682\u001b[0m \u001b[39melif\u001b[39;00m use_fast \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m config_tokenizer_class\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mFast\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    683\u001b[0m     tokenizer_class_candidate \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mconfig_tokenizer_class\u001b[39m}\u001b[39;00m\u001b[39mFast\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:437\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, repo_type, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    425\u001b[0m final_module \u001b[39m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    426\u001b[0m     repo_id,\n\u001b[1;32m    427\u001b[0m     module_file \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m     repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m get_class_in_module(class_name, final_module\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:163\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mImport a module on the cache directory for modules and extract a class from it.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m module_path \u001b[39m=\u001b[39m module_path\u001b[39m.\u001b[39mreplace(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msep, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(module_path)\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/tokenization_kosmos2_fast.py:48\u001b[0m\n\u001b[1;32m     37\u001b[0m PRETRAINED_VOCAB_FILES_MAP \u001b[39m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvocab_file\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m     39\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/kosmos-2-patch14-224\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/sentencepiece.bpe.model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m     }\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES \u001b[39m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/kosmos-2-patch14-224\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2048\u001b[39m,\n\u001b[1;32m     45\u001b[0m }\n\u001b[0;32m---> 48\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mKosmos2TokenizerFast\u001b[39;00m(PreTrainedTokenizerFast):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    Construct a \"fast\" KOSMOS-2 tokenizer (backed by HuggingFace's *tokenizers* library). Adapted from\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m            format `<patch_index_xxxx>` where `xxxx` is an integer.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     vocab_files_names \u001b[39m=\u001b[39m VOCAB_FILES_NAMES\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/tokenization_kosmos2_fast.py:106\u001b[0m, in \u001b[0;36mKosmos2TokenizerFast\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m max_model_input_sizes \u001b[39m=\u001b[39m PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\u001b[1;32m    105\u001b[0m model_input_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 106\u001b[0m slow_tokenizer_class \u001b[39m=\u001b[39m Kosmos2Tokenizer\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m ):\n\u001b[1;32m    123\u001b[0m     \u001b[39m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Kosmos2Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\n",
    "\n",
    "prompt = \"<grounding>An image of\"\n",
    "\n",
    "url = \"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# The original Kosmos-2 demo saves the image first then reload it. For some images, this will give slightly different image input and change the generation outputs.\n",
    "# Uncomment the following 2 lines if you want to match the original demo's outputs.\n",
    "# (One example is the `two_dogs.jpg` from the demo)\n",
    "# image.save(\"new_image.jpg\")\n",
    "# image = Image.open(\"new_image.jpg\")\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"][:, :-1],\n",
    "    attention_mask=inputs[\"attention_mask\"][:, :-1],\n",
    "    img_features=None,\n",
    "    img_attn_mask=inputs[\"img_attn_mask\"][:, :-1],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=64,\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(processed_text)\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(entities)\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
